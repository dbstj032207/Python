{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "font_name = fm.FontProperties(fname=\"C:\\\\Windows\\\\Fonts\\\\malgun.ttf\").get_name()\n",
    "plt.rc(\"font\", family=font_name)\n",
    "\n",
    "import matplotlib as mlp\n",
    "mlp.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "import urllib.request as req\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError, URLError\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import requests\n",
    "\n",
    "import json\n",
    "import re\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json 파일 생성\n",
    "data = {}\n",
    "data[\"data\"] = []\n",
    "\n",
    "with open('recipe_data.json', 'w', encoding=\"utf-8\") as make_file:    \n",
    "    json.dump(data, make_file, ensure_ascii=False, indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff6b96ae8ea48e9a023f3dc140cc0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [104]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m url \u001b[38;5;241m=\u001b[39m theme_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&OrderCondition=&OrderBy=&page=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m      \n\u001b[0;32m     15\u001b[0m url \u001b[38;5;241m=\u001b[39m url\u001b[38;5;241m.\u001b[39mformat(next_page_num_list[k])\n\u001b[1;32m---> 16\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(page\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m, alt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m마지막으로\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):    \n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# 페이지에서 >> 버튼(\"마지막으로\")이 있을경우 다음 큰 페이지가 있음을 의미함\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py:75\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m'\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    524\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m'\u001b[39m: allow_redirects,\n\u001b[0;32m    527\u001b[0m }\n\u001b[0;32m    528\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 529\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:687\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 687\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py:838\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    836\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    837\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 838\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py:760\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    759\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 760\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    761\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\response.py:575\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m--> 575\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content):\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m line\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\response.py:767\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 767\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    769\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\response.py:697\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 697\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    698\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 해당 사이트에 총 25개의 카테고리가 있음을 확인(1 ~ 25)\n",
    "for i in notebook.tqdm(range(25)): # 0 ~ 24\n",
    "    theme_url = \"https://2bob.co.kr/recipe.php?id=list&fKeyList=&fKeyValue=&eTheme={}\"\n",
    "    theme_url = theme_url.format(i+1)\n",
    "\n",
    "\n",
    "    # 한개의카테고리에 몇개의 큰 페이지(5개 묶음 ex> 1 ~ 5, 6 ~ 10, 11 ~ 12)가 있는지 파악\n",
    "    # ex) 3페이지일 경우 리스트가 [1, 6, 11]\n",
    "    # ex) 2페이지일 경우 리스트가 [1. 6]\n",
    "    next_page_num_list = [1]\n",
    "    k = 0\n",
    "    while(True):\n",
    "        # 웹 소스 준비\n",
    "        url = theme_url + \"&OrderCondition=&OrderBy=&page={}\"      \n",
    "        url = url.format(next_page_num_list[k])\n",
    "        page = requests.get(url, headers=header)\n",
    "\n",
    "        soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "        if(soup.find(\"img\", alt=\"마지막으로\") != None):    \n",
    "            # 페이지에서 >> 버튼(\"마지막으로\")이 있을경우 다음 큰 페이지가 있음을 의미함\n",
    "            next_Page = soup.find(\"img\", alt=\"마지막으로\").parent\n",
    "        else:\n",
    "            break   # 마지막으로 버튼이 없을 경우 해당 카테고리의 마지막 큰 페이지이므로 반복문 종료\n",
    "        if(next_page_num_list[k] < int(next_Page[\"href\"].split(\"=\")[-1])):\n",
    "            next_page_num_list.append(int(next_Page[\"href\"].split(\"=\")[-1]))\n",
    "            k = k + 1\n",
    "#             print(next_Page[\"href\"].split(\"=\")[-1])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "#     print(next_page_num_list)\n",
    "\n",
    "    for lead_page_num in notebook.tqdm(next_page_num_list):\n",
    "\n",
    "        # 웹 소스 준비\n",
    "        url = theme_url + \"&OrderCondition=&OrderBy=&page={}\"     \n",
    "        url = url.format(lead_page_num)\n",
    "\n",
    "        header = {\"User-Agent\" : \"Mozilla/5.0\"}\n",
    "        page = requests.get(url, headers=header)\n",
    "\n",
    "        # 가져온 웹 소스를 트리 구조로 변환\n",
    "        soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "        # urljoin을 위한 base 선언\n",
    "        base = \"https://2bob.co.kr\"\n",
    "        base2 = \"https://2bob.co.kr/recipe.php\"\n",
    "\n",
    "        # 한개의 카테고리의 페이지 url 가져오기\n",
    "        page_list = []\n",
    "        page_list.append(url)\n",
    "        page_list.append(soup.find(\"p\", \"paging_num\").a[\"href\"])\n",
    "        next_pages = soup.find(\"p\", \"paging_num\").a.next_siblings\n",
    "\n",
    "        for page in next_pages:\n",
    "            page_list.append(page[\"href\"])\n",
    "\n",
    "        for page in notebook.tqdm(page_list):\n",
    "            if(page == url):\n",
    "                url = page\n",
    "            else:\n",
    "                url = base2 + page\n",
    "\n",
    "            page = requests.get(url, headers=header)\n",
    "\n",
    "            # 가져온 웹 소스를 트리 구조로 변환\n",
    "            soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "            # 페이지당 4줄, 4줄의 맨 앞에있는 레시피만 ml_none 클래스 부여되어있음\n",
    "            # 따라서 뒤에 따라오는 4개의 레시피는 해당 레시피 기준 siblings을 사용하여 가져옴\n",
    "\n",
    "            # 가로기준 맨 앞 항목의 링크 가져오기\n",
    "            urls = soup.select('li.ml_none > a')\n",
    "            lead_urls_list = []\n",
    "            for url in urls:\n",
    "                lead_urls_list.append(url[\"href\"])\n",
    "\n",
    "            # 맨 앞 항목의 링크를 기준으로 따라오는 4개의 링크 가져오기\n",
    "            follow_url_list=[]\n",
    "            for lead_url in lead_urls_list:\n",
    "                urls = soup.find(\"a\", href=lead_url).parent.next_siblings\n",
    "                for url in urls:\n",
    "                    if(type(url) == bs4.element.Tag):\n",
    "                        follow_url_list.append(list(url.children)[1][\"href\"])\n",
    "\n",
    "            # 두개의 리스트 합치기\n",
    "            url_list = lead_urls_list + follow_url_list\n",
    "\n",
    "            # 합쳐진 리스트의 중복 제거\n",
    "            url_list = set(url_list)\n",
    "            url_list = list(url_list)\n",
    "\n",
    "            # 괄호, \\r\\n 제거를 위한 패턴 생성\n",
    "            pattern1 = r'\\([^)]*\\)'\n",
    "            pattern2 = '\\r\\n'\n",
    "            pattern3 = '\\n'\n",
    "\n",
    "            # 생성해 놓은 json파일 load\n",
    "            with open('recipe_data.json', \"r\", encoding='utf-8') as json_file:    \n",
    "                json_data = json.load(json_file)\n",
    "\n",
    "\n",
    "\n",
    "            # 각자의 url별로 크롤링 실행\n",
    "            # cnt 값 매번 조정 필요\n",
    "            # 이윤석 0-999\n",
    "            # 송지민 1000-1999\n",
    "            # 홍진우 2000-2999\n",
    "            # 전우진 3000-3999                      \n",
    "            if(len(json_data[\"data\"]) == 0): # 처음 저장할 경우\n",
    "                cnt = 0\n",
    "            else: # 기존에 테이터가 있을 경우\n",
    "                cnt = json_data[\"data\"][-1][\"code\"] + 1   \n",
    "\n",
    "            for i in notebook.tqdm(url_list):\n",
    "                # 웹 소스 준비\n",
    "                url = urllib.parse.urljoin(base, i)\n",
    "\n",
    "                header = {\"User-Agent\" : \"Mozilla/5.0\"}\n",
    "                page = requests.get(url, headers=header)\n",
    "                page\n",
    "                # 가져온 웹 소스를 트리 구조로 변환\n",
    "                soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "                # 레시피 명\n",
    "                title = soup.find('h2', class_=\"prod_title\").get_text()\n",
    "#                 print(title)\n",
    "\n",
    "                # 필수재료\n",
    "                ingredients = soup.find('p', class_=\"mate_list\").get_text()\n",
    "\n",
    "                # 재료명에서 괄호와 괄호안의 정보 삭제\n",
    "                ingredients = re.sub(pattern=pattern1, repl='', string=ingredients)\n",
    "\n",
    "                # 재료명에서 \\r\\n 제거\n",
    "                ingredients = re.sub(pattern=pattern2, repl='', string=ingredients)\n",
    "                \n",
    "                # 재료명에서 \\n 제거\n",
    "                ingredients = re.sub(pattern=pattern3, repl='', string=ingredients)\n",
    "\n",
    "                # 리스트로 변환\n",
    "                temp_list = ingredients.split(',')\n",
    "\n",
    "                ingredients_list = []\n",
    "                # 공백 제거\n",
    "                for i in temp_list:\n",
    "                    i = re.sub(\" \", \"\", string=i)\n",
    "                    ingredients_list.append(i)\n",
    "\n",
    "#                 print(ingredients_list)\n",
    "\n",
    "\n",
    "                # 양념\n",
    "                siblings = soup.find('p', class_=\"mate_list\").parent.next_siblings\n",
    "                for sibling in siblings:\n",
    "                    if(type(sibling) == bs4.element.Tag):\n",
    "                        print(sibling)\n",
    "                        print(\"--------\")\n",
    "\n",
    "                        # 필수재료\n",
    "                        ingredients = sibling.p.get_text()\n",
    "\n",
    "                        # 재료명에서 괄호와 괄호안의 정보 삭제\n",
    "                        ingredients = re.sub(pattern=pattern1, repl='', string=ingredients)\n",
    "\n",
    "                        # 재료명에서 \\r\\n 제거\n",
    "                        ingredients = re.sub(pattern=pattern2, repl='', string=ingredients)\n",
    "\n",
    "                        # 재료명에서 \\n 제거\n",
    "                        ingredients = re.sub(pattern=pattern3, repl='', string=ingredients)\n",
    "\n",
    "                        # 리스트로 변환\n",
    "                        temp_list = ingredients.split(',')\n",
    "\n",
    "\n",
    "                        # 공백 제거\n",
    "                        for i in temp_list:\n",
    "                            i = re.sub(\" \", \"\", string=i)\n",
    "                            ingredients_list.append(i)\n",
    "                \n",
    "                # 특수 양념 예외처리\n",
    "                for ingredient in ingredients_list:\n",
    "                    if(\"+\" in ingredient):\n",
    "                        temp = ingredient.split(\"+\")\n",
    "                        ingredients_list.remove(ingredient)\n",
    "                        ingredients_list = ingredients_list + temp\n",
    "    \n",
    "                json_data['data'].append({\n",
    "                    \"code\" : cnt,\n",
    "                    \"title\" : title,\n",
    "                    \"ingredients\" : ingredients_list,\n",
    "                    \"url\" : url\n",
    "                })\n",
    "#                 print(\"--------------------------------------------------\")\n",
    "                cnt = cnt + 1\n",
    "                # json 파일 저장\n",
    "                with open('recipe_data.json', \"w\", encoding='utf-8') as outfile:      ####################################### 수정해야할 곳 ####################\n",
    "                    json.dump(json_data, outfile, indent=\"\\t\", ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3abef859d002c6750ca6e31532104864d50c2c03ecac513d70642831473e898a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
